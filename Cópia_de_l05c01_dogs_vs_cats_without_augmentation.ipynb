{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pfanio/acelEY/blob/main/C%C3%B3pia_de_l05c01_dogs_vs_cats_without_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBFXQGKYUc4X"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1z4xy2gTUc4a"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE7KNzPPVrVV"
      },
      "source": [
        "# Dogs vs Cats Image Classification Without Image Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwQtSOz0VrVX"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l05c01_dogs_vs_cats_without_augmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l05c01_dogs_vs_cats_without_augmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN7G9GFmVrVY"
      },
      "source": [
        "In this tutorial, we will discuss how to classify images into pictures of cats or pictures of dogs. We'll build an image classifier using `tf.keras.Sequential` model and load data using `tf.keras.preprocessing.image.ImageDataGenerator`.\n",
        "\n",
        "## Specific concepts that will be covered:\n",
        "In the process, we will build practical experience and develop intuition around the following concepts\n",
        "\n",
        "* Building _data input pipelines_ using the `tf.keras.preprocessing.image.ImageDataGenerator` class — How can we efficiently work with data on disk to interface with our model?\n",
        "* _Overfitting_ - what is it, how to identify it?\n",
        "\n",
        "<hr>\n",
        "\n",
        "\n",
        "**Before you begin**\n",
        "\n",
        "Before running the code in this notebook, reset the runtime by going to **Runtime -> Reset all runtimes** in the menu above. If you have been working through several notebooks, this will help you avoid reaching Colab's memory limits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF9uvbXNVrVY"
      },
      "source": [
        "# Importing packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VddxeYBEVrVZ"
      },
      "source": [
        "Let's start by importing required packages:\n",
        "\n",
        "*   os — to read files and directory structure\n",
        "*   numpy — for some matrix math outside of TensorFlow\n",
        "*   matplotlib.pyplot — to plot the graph and display images in our training and validation data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSdjGwVWGshH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlORkUyFGxWH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqtiIPRbG4FA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHHqtPisG3R1"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZZI6lNkVrVm"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPHx8-t-VrVo"
      },
      "source": [
        "To build our image classifier, we begin by downloading the dataset. The dataset we are using is a filtered version of <a href=\"https://www.kaggle.com/c/dogs-vs-cats/data\" target=\"_blank\">Dogs vs. Cats</a> dataset from Kaggle (ultimately, this dataset is provided by Microsoft Research).\n",
        "\n",
        "In previous Colabs, we've used <a href=\"https://www.tensorflow.org/datasets\" target=\"_blank\">TensorFlow Datasets</a>, which is a very easy and convenient way to use datasets. In this Colab however, we will make use of the class `tf.keras.preprocessing.image.ImageDataGenerator` which will read data from disk. We therefore need to directly download *Dogs vs. Cats* from a URL and unzip it to the Colab filesystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpUSoFjuVrVp"
      },
      "outputs": [],
      "source": [
        "#_URL = 'Libras_Dataset_Reduzido.zip'\n",
        "zip_dir = '/content/drive/MyDrive/Colab_Notebooks/Python_ML/Libras_Dataset_Reduzido'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Giv0wMQzVrVw"
      },
      "source": [
        "The dataset we have downloaded has the following directory structure.\n",
        "\n",
        "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
        "<b>cats_and_dogs_filtered</b>\n",
        "|__ <b>train</b>\n",
        "    |______ <b>cats</b>: [cat.0.jpg, cat.1.jpg, cat.2.jpg ...]\n",
        "    |______ <b>dogs</b>: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]\n",
        "|__ <b>validation</b>\n",
        "    |______ <b>cats</b>: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ...]\n",
        "    |______ <b>dogs</b>: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]\n",
        "</pre>\n",
        "\n",
        "We can list the directories with the following terminal command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssD23VbTZeVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "982d72f7-2d2c-4932-93ee-e57d4a42bb84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/Python_ML\n"
          ]
        }
      ],
      "source": [
        "zip_dir_base = os.path.dirname(zip_dir)\n",
        "print(zip_dir_base)\n",
        "#!find $zip_dir_base -type d -print"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TYz5Te4RIim9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpmywIlsVrVx"
      },
      "source": [
        "We'll now assign variables with the proper file path for the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRucI3QqVrVy"
      },
      "outputs": [],
      "source": [
        "base_dir = os.path.join(os.path.dirname(zip_dir), 'Libras_Dataset_Reduzido')\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "train_A_dir = os.path.join(train_dir, 'A')  \n",
        "train_B_dir = os.path.join(train_dir, 'B')\n",
        "train_C_dir = os.path.join(train_dir, 'C') \n",
        "train_D_dir = os.path.join(train_dir, 'D') \n",
        "train_E_dir = os.path.join(train_dir, 'E') \n",
        "train_F_dir = os.path.join(train_dir, 'F') \n",
        "train_G_dir = os.path.join(train_dir, 'G') \n",
        "train_I_dir = os.path.join(train_dir, 'I') \n",
        "train_L_dir = os.path.join(train_dir, 'L') \n",
        "train_M_dir = os.path.join(train_dir, 'M') \n",
        "train_N_dir = os.path.join(train_dir, 'N') \n",
        "train_O_dir = os.path.join(train_dir, 'O') \n",
        "train_P_dir = os.path.join(train_dir, 'P') \n",
        "train_Q_dir = os.path.join(train_dir, 'Q') \n",
        "train_R_dir = os.path.join(train_dir, 'R') \n",
        "train_S_dir = os.path.join(train_dir, 'S') \n",
        "train_T_dir = os.path.join(train_dir, 'T') \n",
        "train_U_dir = os.path.join(train_dir, 'U') \n",
        "train_V_dir = os.path.join(train_dir, 'V') \n",
        "train_W_dir = os.path.join(train_dir, 'W') \n",
        "train_X_dir = os.path.join(train_dir, 'X')   \n",
        "train_Y_dir = os.path.join(train_dir, 'Y') \n",
        "\n",
        "validation_A_dir = os.path.join(validation_dir, 'A')  \n",
        "validation_B_dir = os.path.join(validation_dir, 'B')\n",
        "validation_C_dir = os.path.join(validation_dir, 'C') \n",
        "validation_D_dir = os.path.join(validation_dir, 'D') \n",
        "validation_E_dir = os.path.join(validation_dir, 'E') \n",
        "validation_F_dir = os.path.join(validation_dir, 'F') \n",
        "validation_G_dir = os.path.join(validation_dir, 'G') \n",
        "validation_I_dir = os.path.join(validation_dir, 'I') \n",
        "validation_L_dir = os.path.join(validation_dir, 'L') \n",
        "validation_M_dir = os.path.join(validation_dir, 'M') \n",
        "validation_N_dir = os.path.join(validation_dir, 'N') \n",
        "validation_O_dir = os.path.join(validation_dir, 'O') \n",
        "validation_P_dir = os.path.join(validation_dir, 'P') \n",
        "validation_Q_dir = os.path.join(validation_dir, 'Q') \n",
        "validation_R_dir = os.path.join(validation_dir, 'R') \n",
        "validation_S_dir = os.path.join(validation_dir, 'S') \n",
        "validation_T_dir = os.path.join(validation_dir, 'T') \n",
        "validation_U_dir = os.path.join(validation_dir, 'U') \n",
        "validation_V_dir = os.path.join(validation_dir, 'V') \n",
        "validation_W_dir = os.path.join(validation_dir, 'W') \n",
        "validation_X_dir = os.path.join(validation_dir, 'X')   \n",
        "validation_Y_dir = os.path.join(validation_dir, 'Y') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdrHHTy2VrV3"
      },
      "source": [
        "### Understanding our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LblUYjl-VrV3"
      },
      "source": [
        "Let's look at how many cats and dogs images we have in our training and validation directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc4u8e9hVrV4"
      },
      "outputs": [],
      "source": [
        "num_A_tr = len(os.listdir(train_A_dir))\n",
        "num_B_tr = len(os.listdir(train_B_dir))\n",
        "\n",
        "num_A_val = len(os.listdir(validation_A_dir))\n",
        "num_B_val = len(os.listdir(validation_B_dir))\n",
        "\n",
        "total_train = num_A_tr + num_B_tr\n",
        "total_val = num_A_val + num_B_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4GGzGt0VrV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b2a9549-4d4b-413c-8ddc-d210e034bf85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training A images: 240\n",
            "total training B images: 240\n",
            "total validation A images: 40\n",
            "total validation B images: 40\n",
            "--\n",
            "Total training images: 480\n",
            "Total validation images: 80\n"
          ]
        }
      ],
      "source": [
        "print('total training A images:', num_A_tr)\n",
        "print('total training B images:', num_B_tr)\n",
        "\n",
        "print('total validation A images:', num_A_val)\n",
        "print('total validation B images:', num_B_val)\n",
        "print(\"--\")\n",
        "print(\"Total training images:\", total_train)\n",
        "print(\"Total validation images:\", total_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdsI_L-NVrV_"
      },
      "source": [
        "# Setting Model Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lp-0ejxOtP1"
      },
      "source": [
        "For convenience, we'll set up variables that will be used later while pre-processing our dataset and training our network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NqNselLVrWA"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 100  # Number of training examples to process before updating our models variables\n",
        "IMG_SHAPE  = 50  # Our training data consists of images with width of 50 pixels and height of 50 pixels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INn-cOn1VrWC"
      },
      "source": [
        "# Data Preparation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jfk6aSAVrWD"
      },
      "source": [
        "Images must be formatted into appropriately pre-processed floating point tensors before being fed into the network. The steps involved in preparing these images are:\n",
        "\n",
        "1. Read images from the disk\n",
        "2. Decode contents of these images and convert it into proper grid format as per their RGB content\n",
        "3. Convert them into floating point tensors\n",
        "4. Rescale the tensors from values between 0 and 255 to values between 0 and 1, as neural networks prefer to deal with small input values.\n",
        "\n",
        "Fortunately, all these tasks can be done using the class **tf.keras.preprocessing.image.ImageDataGenerator**.\n",
        "\n",
        "We can set this up in a couple of lines of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syDdF_LWVrWE"
      },
      "outputs": [],
      "source": [
        "train_image_generator      = ImageDataGenerator(rescale=0)  # Generator for our training data\n",
        "validation_image_generator = ImageDataGenerator(rescale=0)  # Generator for our validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLciCR_FVrWH"
      },
      "source": [
        "After defining our generators for training and validation images, **flow_from_directory** method will load images from the disk, apply rescaling, and resize them using single line of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw94ajOOVrWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0830f0a5-ce0e-49a7-db20-f0d9216ac576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5280 images belonging to 22 classes.\n"
          ]
        }
      ],
      "source": [
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_SHAPE,IMG_SHAPE), #(50,50)\n",
        "                                                           class_mode='binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oUoKUzRVrWM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9417c3a5-08a1-4c8d-e23b-73ce13a85f41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 880 images belonging to 22 classes.\n"
          ]
        }
      ],
      "source": [
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              shuffle=False,\n",
        "                                                              target_size=(IMG_SHAPE,IMG_SHAPE), #(50,50)\n",
        "                                                              class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyexPJ8CVrWP"
      },
      "source": [
        "### Visualizing Training images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60CnhEL4VrWQ"
      },
      "source": [
        "We can visualize our training images by getting a batch of images from the training generator, and then plotting a few of them using `matplotlib`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f0Z7NZgVrWQ"
      },
      "outputs": [],
      "source": [
        "sample_training_images, _ = next(train_data_gen) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49weMt5YVrWT"
      },
      "source": [
        "The `next` function returns a batch from the dataset. One batch is a tuple of (*many images*, *many labels*). For right now, we're discarding the labels because we just want to look at the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMt2RES_VrWU"
      },
      "outputs": [],
      "source": [
        "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
        "def plotImages(images_arr):\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
        "    axes = axes.flatten()\n",
        "    for img, ax in zip(images_arr, axes):\n",
        "        ax.imshow(img)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_VVg_gEVrWW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "4be684f1-85f2-416b-f5ff-116ff8ee6ed7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 5 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABZgAAAEgCAYAAAA5exbPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYlElEQVR4nO3dUahs13kf8P9XXbsOhCA7vgiha3pdLBr00Nj4Yhzch6LUoDoh0oMpNqHoQaCXBBwaSJUECoE+xC9x8tAXERvpIcROnYCECRRVUQiFIvsqdlLbIpFibCIjW1fEIslLWiWrD2cUHd87J2dmzcw6e8/+/WBzZ/bZM7PWzJ4/R99Z2l+11gIAAAAAANv6Zxc9AAAAAAAA5kmBGQAAAACALgrMAAAAAAB0UWAGAAAAAKCLAjMAAAAAAF0UmAEAAAAA6LJTgbmq7quqP6uqF6vqkX0NCuAscgcYTe4Ao8kdYDS5A+yiWmt9D6y6LcmfJ/lwkpeSfCnJx1trXz/rMe985zvb1atXu14PmK9vfvObefXVV2vX55E7wKbkDjDaReWOzIHleu65515trV3e9Xm2zZ2q6isk8X3e//73X/QQWIDnnntuo+PWnY9nPHZt7lzadmCnfCDJi621byRJVX02yf1JzvwPrqtXr+b69es7vCQwR9euXdvXU8kdYCNyBxjtonJH5sByVdW39vRUW/++w+5kNyNUbfa373Xn4xmPXZs7u1wi464kf3nq/kurfQCHIneA0eQOMJrcAUaTO8BODt7kr6oerqrrVXX9xo0bh345ALkDDCd3gJFkDjDa6dy56LEA07NLgfnbSd516v6V1b7v01p7tLV2rbV27fLlnS8NBCyb3AFGkzvAaOfmjswB9myr3Bk6MmAWdikwfynJ3VX17qp6a5KPJXlyP8MCWEvuAKPJHWA0uQOMtojcaa1ttG3z2H2rqls2lm3dOXGI82TX5+xu8tdae72qfjbJ/0hyW5LPtNa+1vt8AOeRO8BocgcYTe4Ao8kdYFfdBeYkaa39fpLf39NYAM4ld4DR5A4wmtwBRpM7wC4O3uQPAAAAAIDjpMAMAAAAAECXnS6RAQAAAAAXZdNme7s0Qjvrsftu9HeIxoEwghXMAAAAAAB0UWAGAAAAAKCLAjMAAAAAAF0UmAEAAAAA6KLJHwAAAACztK4B37pmeev2bfrYs5r8bdo4UPO+Zdv3ebLNObrNsbuwghkAAAAAgC4KzAAAAAAAdFFgBgAAAACgiwIzAAAAAABdNPkDAAAAYJY2bYy2a0O/fY+H+dt3875DNN8bxQpmAAAAAAC6KDADAAAAANBFgRkAAAAAgC4KzAAAAAAAdFFgBgAAAACgy6WLHgAAAAAA9KiqjY5rrW302HXH7frazNtZn/M258qmz7nJa2xz3o46R61gBgAAAACgiwIzAAAAAABdFJgBAAAAAOiiwAwAAAAAQBdN/gAAAAA4Gps2Rltnm6ZouzR5Yz62aaC3y7l3iPN2l+fchhXMAAAAAAB0UWAGAAAAAKCLAjMAAAAAAF0UmAEAAAAA6KLJHwAAAACztGkTs10a8h2iKRrzsc3nv++Gfrs2khx17lrBDAAAAABAFwVmAAAAAAC6KDADAAAAANBFgRkAAAAAgC6a/AEAAAAwS5s2Rtv3cTDiPNvluG2P3YUVzAAAAAAAdFFgBgAAAACgiwIzAAAAAABdFJgBAAAAAOiiyR8AAAAAs7TvRn0a+nGzs86JTZvlXdRx2x67CyuYAQAAAADoosAMAAAAAEAXBWYAAAAAALqcW2Cuqs9U1StV9dVT+95RVU9V1Qurf99+2GECSyJ3gNHkDjCa3AFGkzvAoWyygvmxJPfdtO+RJE+31u5O8vTqPsC+PBa5A4z1WOQOMNZjkTvAWI9F7nyfqrplY9nWnRNnba21W7alOrfA3Fr7oyR/ddPu+5M8vrr9eJIH9jwuYMHkDjCa3AFGkzvAaHIHOJTeazDf0Vp7eXX7O0nu2NN4AM4id4DR5A4wmtwBRpM7wM52bvLXTtZ/n7kGvKoerqrrVXX9xo0bu74cgNwBhpM7wGj/VO7IHOAQNs2dwcMCZqC3wPzdqrozSVb/vnLWga21R1tr11pr1y5fvtz5cgByBxhO7gCjbZQ7MgfYo61zZ+jogFnoLTA/meTB1e0Hkzyxn+EAnEnuAKPJHWA0uQOMdpS5s675moZ+9Fp3Pi25od865xaYq+q3k/zvJP+qql6qqoeS/GqSD1fVC0n+3eo+wF7IHWA0uQOMJneA0eQOcCiXzjugtfbxM37043seC0ASuQOMJ3eA0eQOMJrcAQ5l5yZ/AAAAAAAskwIzAAAAAABdzr1EBgAAAABM0bpmfesasGnKRi8NIc9nBTMAAAAAAF0UmAEAAAAA6KLADAAAAABAFwVmAAAAAAC6KDADAAAAANDl0kUPAAAAAAB6tNZu2VdVez2O5dj18193Ti2BFcwAAAAAAHRRYAYAAAAAoIsCMwAAAAAAXRSYAQAAAADooskfAAAAAEdtqc3X2J1z53xWMAMAAAAA0EWBGQAAAACALgrMAAAAAAB0UWAGAAAAAKCLJn8AAAAAzNK6BmyttQsYCUuz7jxbakNAK5gBAAAAAOiiwAwAAAAAQBcFZgAAAAAAuigwAwAAAADQRZM/AAAAAGZpl4Z+GgSyi6U29FvHCmYAAAAAALooMAMAAAAA0EWBGQAAAACALgrMAAAAAAB00eQPAAAAgFnSaI192qbxo3PvTVYwAwAAAADQRYEZAAAAAIAuCswAAAAAAHRRYAYAAAAAoIsmfwAAAADAouzapG9d87+lNv6zghkAAAAAgC4KzAAAAAAAdFFgBgAAAACgiwIzAAAAAABdNPkDAAAAABZl0yZ9ZzXuW/f4pbKCGQAAAACALgrMAAAAAAB0UWAGAAAAAKDLuQXmqnpXVT1TVV+vqq9V1SdW+99RVU9V1Qurf99++OECSyB3gNHkDjCSzAFGkzvAIW2ygvn1JD/fWrsnyQeT/ExV3ZPkkSRPt9buTvL06j7APsgdYDS5A4wkc4DR5A7cpKpu2dZpra3dNn38EpxbYG6tvdxa++PV7b9J8nySu5Lcn+Tx1WGPJ3ngUIMElkXuAKPJHWAkmQOMJneAQ9rqGsxVdTXJ+5I8m+SO1trLqx99J8kdex0ZQOQOMJ7cAUaSOcBocgfYt40LzFX1g0l+N8nPtdb++vTPWmstSTvjcQ9X1fWqun7jxo2dBgssi9wBRpM7wEgyBxhtH7kzYJjAzGxUYK6qt+QkgH6rtfZ7q93frao7Vz+/M8kr6x7bWnu0tXattXbt8uXL+xgzsAByBxhN7gAjyRxgtH3lzpjRAnNyboG5Tq5Q/ekkz7fWfu3Uj55M8uDq9oNJntj/8IAlkjvAaHIHGEnmAKPJHeCQLm1wzIeS/Mck/6eqvrLa90tJfjXJ71TVQ0m+leQ/HGaIwALJHWA0uQOMJHOA0eQO3OTkqjDf7+RvMefv2+bxS3Bugbm19r+SnPXu/Ph+hwMgd4Dx5A4wkswBRpM7wCFt3OQPAAAAAABOU2AGAAAAAKCLAjMAAAAAAF02afIHAAAAADBLmzbf26Zx31Ib+q1jBTMAAAAAAF0UmAEAAAAA6KLADAAAAABAFwVmAAAAAAC6aPIHAAAAACzKuoZ+uz5+qY3/rGAGAAAAAKCLAjMAAAAAAF0UmAEAAAAA6KLADAAAAABAF03+AAAAAIDFW9ekb9dmgEtgBTMAAAAAAF0UmAEAAAAA6KLADAAAAABAFwVmAAAAAAC6aPIHAAAAwNFY15RN87Zl2/ScWOes45w/b7KCGQAAAACALgrMAAAAAAB0UWAGAAAAAKCLAjMAAAAAAF00+QMAAABglnZp6KfxHzfbpRngklnBDAAAAABAFwVmAAAAAAC6KDADAAAAANBFgRkAAAAAgC6a/AEAAAAwS7s0YNPQb9l2/fw1/3uTFcwAAAAAAHRRYAYAAAAAoIsCMwAAAAAAXRSYAQAAAADooskfAAAAALO0aaO2TRuyafx3nEY1g1xq4z8rmAEAAAAA6KLADAAAAABAFwVmAAAAAAC6KDADAAAAANBFkz8AAAAAZmnfzfu2adKmIeB8+KwOywpmAAAAAAC6KDADAAAAANBFgRkAAAAAgC7nFpir6m1V9cWq+pOq+lpV/cpq/7ur6tmqerGqPldVbz38cIElkDvAaHIHGE3uACPJHOCQNlnB/HdJ7m2t/WiS9ya5r6o+mOSTST7VWntPku8leehwwwQWRu4Ao8kdYDS5A4wkc4CDObfA3E787eruW1ZbS3Jvks+v9j+e5IGDjBBYHLkDjCZ3gNHkDjDSMWdOa+2WbVNVdcu27vnO2piPdZ/1Lsdte+yx2+gazFV1W1V9JckrSZ5K8hdJXmutvb465KUkdx1miMASyR1gNLkDjCZ3gJFkDnAoGxWYW2t/31p7b5IrST6Q5Ec2fYGqeriqrlfV9Rs3bnQOE1gauQOMJneA0XpzR+YAPfb1u87BBgjM1kYF5je01l5L8kySH0tye1VdWv3oSpJvn/GYR1tr11pr1y5fvrzTYIHlkTvAaHIHGG3b3JE5wC52/V1n0DCBGTm3wFxVl6vq9tXtH0jy4STP5ySMPro67MEkTxxqkMCyyB1gNLkDjCZ3gJFkDnBIl84/JHcmebyqbstJQfp3WmtfqKqvJ/lsVf3XJF9O8ukDjhNYFrkDjCZ3gNHkDjDS0WbOusZquzTg26ZRm0Z/y3DWObHu819qo79zC8yttT9N8r41+7+Rk2v2AOyV3AFGkzvAaHIHGEnmAIe01TWYAQAAAADgDQrMAAAAAAB0UWAGAAAAAKDLJk3+AAAAAGAWdmm0tk3jvn03GORwNm3It+vnv1RWMAMAAAAA0EWBGQAAAACALgrMAAAAAAB0UWAGAAAAAKCLJn8AAAAAkN0bt236eM0A50Mzv/NZwQwAAAAAQBcFZgAAAAAAuigwAwAAAADQRYEZAAAAAIAumvwBAAAAAIu3TUO/dY0al9oQ0ApmAAAAAAC6KDADAAAAANBFgRkAAAAAgC4KzAAAAAAAdNHkDwAAAAAOZNNmcOuOY6xtPoOlNvRbxwpmAAAAAAC6KDADAAAAANBFgRkAAAAAgC4KzAAAAAAAdNHkDwAAAAAOREO/sTZtvrfpZ3DW823avHEJrGAGAAAAAKCLAjMAAAAAAF0UmAEAAAAA6KLADAAAAABAF03+AAAAAGAgjf8OZ5fme0tt0rcrK5gBAAAAAOiiwAwAAAAAQBcFZgAAAAAAuigwAwAAAADQRYEZAAAAAIAuly56AAAAAABwrFprt+yrqgsYyTKse2/XfQbrbHrcWa+zVFYwAwAAAADQRYEZAAAAAIAuCswAAAAAAHRRYAYAAAAAoIsmfwAAAABwILs0nWN7mzZV3LX5ouaNb7KCGQAAAACALgrMAAAAAAB0UWAGAAAAAKDLxgXmqrqtqr5cVV9Y3X93VT1bVS9W1eeq6q2HGyawRHIHGEnmAKPJHWA0uQMcwjYrmD+R5PlT9z+Z5FOttfck+V6Sh/Y5MIDIHWAsmQOMJneA0eQOR6+qbtlaa7ds645bZ91jt3n8EmxUYK6qK0l+Islvru5XknuTfH51yONJHjjEAIFlkjvASDIHGE3uAKPJHeBQNl3B/OtJfiHJP6zu/3CS11prr6/uv5TkrnUPrKqHq+p6VV2/cePGToMFFkXuACN1Z04id4AuftcBRttL7hx+mMDcnFtgrqqfTPJKa+25nhdorT3aWrvWWrt2+fLlnqcAFkbuACPtmjmJ3AG243cdYLR95s6ehwYcgUsbHPOhJD9VVR9J8rYkP5TkN5LcXlWXVn/pupLk24cbJrAwcgcYSeYAo8kdYDS5AxzMuSuYW2u/2Fq70lq7muRjSf6gtfbTSZ5J8tHVYQ8meeJgowQWRe4AI8kcYDS5A4wmd2Az65r5cb5Nr8G8zn9O8p+q6sWcXLfn0/sZEsCZ5A4wkswBRpM7wGhyB9jZJpfI+EettT9M8oer299I8oH9DwngTXIHGEnmAKPJHWA0uQPs2y4rmAEAAAAAWDAFZgAAAAAAumx1iQwAAAAAYHMaxY21y/tdVXscyXJYwQwAAAAAQBcFZgAAAAAAuigwAwAAAADQRYEZAAAAAIAumvwBAAAAAEdh00Z9uzZf1BDwTVYwAwAAAADQRYEZAAAAAIAuCswAAAAAAHRRYAYAAAAAoIsmfwAAAADAUdi0ed+uzQDX7V9q4z8rmAEAAAAA6KLADAAAAABAFwVmAAAAAAC6KDADAAAAANBFkz8AAAAAYHbWNdXbtMnfvpsBLpkVzAAAAAAAdFFgBgAAAACgiwIzAAAAAABdFJgBAAAAAOiiwAwAAAAAQJdLFz0AAAAAAIBDqaqNjmutbfyc647d9HWOjRXMAAAAAAB0UWAGAAAAAKCLAjMAAAAAAF0UmAEAAAAA6KLJHwAAAABwFNY12tOQ77CsYAYAAAAAoIsCMwAAAAAAXRSYAQAAAADoosAMAAAAAEAXTf4AAAAA4EA2bTrHP22X5n0a+h2WFcwAAAAAAHRRYAYAAAAAoIsCMwAAAAAAXRSYAQAAAADooskfAAAAAFywuTYD3KWB3lnz2/Q59928b5vn0zjwTVYwAwAAAADQRYEZAAAAAIAuCswAAAAAAHRRYAYAAAAAoEuNvFh4Vd1I8q0k70zy6rAXPqxjmktyXPMxl+n4F621yxfxwnJn8sxluuY+H7mzX+YyTeYyLReSO6cyJzmO9/EN5jJN5jItcme/zGWazGVa1ubO0ALzP75o1fXW2rXhL3wAxzSX5LjmYy6cdkzvoblM0zHNJTm++VyEY3oPzWWazIWbHdP7aC7TZC7c7JjeR3OZJnOZB5fIAAAAAACgiwIzAAAAAABdLqrA/OgFve4hHNNckuOaj7lw2jG9h+YyTcc0l+T45nMRjuk9NJdpMhdudkzvo7lMk7lws2N6H81lmsxlBi7kGswAAAAAAMyfS2QAAAAAANBleIG5qu6rqj+rqher6pHRr7+LqvpMVb1SVV89te8dVfVUVb2w+vftFznGTVXVu6rqmar6elV9rao+sdo/u/lU1duq6otV9SerufzKav+7q+rZ1bn2uap660WPdVNVdVtVfbmqvrC6P9u5TIHcmQa5M21yZ7/kzjTInWmTO/sz58xJ5M5UHVvuyJz9kjvTIHOmbUm5M7TAXFW3JflvSf59knuSfLyq7hk5hh09luS+m/Y9kuTp1trdSZ5e3Z+D15P8fGvtniQfTPIzq89ijvP5uyT3ttZ+NMl7k9xXVR9M8skkn2qtvSfJ95I8dIFj3NYnkjx/6v6c53Kh5M6kyJ1pkzt7IncmRe5Mm9zZgyPInETuTNWx5Y7M2RO5MykyZ9oWkzujVzB/IMmLrbVvtNb+b5LPJrl/8Bi6tdb+KMlf3bT7/iSPr24/nuSBoYPq1Fp7ubX2x6vbf5OTE/6uzHA+7cTfru6+ZbW1JPcm+fxq/yzmkiRVdSXJTyT5zdX9ykznMhFyZyLkznTJnb2TOxMhd6ZL7uzVrDMnkTtTdUy5I3P2Tu5MhMyZrqXlzugC811J/vLU/ZdW++bsjtbay6vb30lyx0UOpkdVXU3yviTPZqbzWf1vB19J8kqSp5L8RZLXWmuvrw6Z07n260l+Ick/rO7/cOY7lymQOxMkdyZH7uyX3JkguTM5cmd/jjFzkpl+T0+TO5Mic/ZL7kyQzJmcReWOJn971FprOfnrymxU1Q8m+d0kP9da++vTP5vTfFprf99ae2+SKzn5a+qPXPCQulTVTyZ5pbX23EWPhXmY0/f0DXJnWuQO25rT9/QNcmda5A7bmtP39A1yZzpkDj3m9D1NZM7ULDF3Lg1+vW8nedep+1dW++bsu1V1Z2vt5aq6Myd/ZZmFqnpLTgLot1prv7faPdv5JElr7bWqeibJjyW5vaourf46NJdz7UNJfqqqPpLkbUl+KMlvZJ5zmQq5MyFyZ5Lkzv7JnQmRO5Mkd/brGDMnmfH3VO5MjszZP7kzITJnkhaXO6NXMH8pyd2rrolvTfKxJE8OHsO+PZnkwdXtB5M8cYFj2djq2i+fTvJ8a+3XTv1odvOpqstVdfvq9g8k+XBOrjv0TJKPrg6bxVxaa7/YWrvSWruak+/HH7TWfjoznMuEyJ2JkDvTJHcOQu5MhNyZJrmzd8eYOckMv6eJ3JkimXMQcmciZM40LTJ3WmtDtyQfSfLnObmOyi+Pfv0dx/7bSV5O8v9ycq2Uh3JyDZWnk7yQ5H8mecdFj3PDufybnPwvEn+a5Cur7SNznE+Sf53ky6u5fDXJf1nt/5dJvpjkxST/Pck/v+ixbjmvf5vkC8cwl4ve5M40Nrkz/U3u7PW9lDsT2OTO9De5s7f3cbaZsxq/3Jngdoy5I3P2+l7KnQlsMmf621Jyp1YTBAAAAACArWjyBwAAAABAFwVmAAAAAAC6KDADAAAAANBFgRkAAAAAgC4KzAAAAAAAdFFgBgAAAACgiwIzAAAAAABdFJgBAAAAAOjy/wG3/DElnm7svAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plotImages(sample_training_images[:5])  # Plot images 0-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5Ej-HLGVrWZ"
      },
      "source": [
        "# Model Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEgW4i18VrWZ"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "The model consists of four convolution blocks with a max pool layer in each of them. Then we have a fully connected layer with 512 units, with a `relu` activation function. The model will output class probabilities for two classes — dogs and cats — using `softmax`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F15-uwLPVrWa"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(50, 50, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(2)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI5cdkMQVrWc"
      },
      "source": [
        "### Compile the model\n",
        "\n",
        "As usual, we will use the `adam` optimizer. Since we output a softmax categorization, we'll use `sparse_categorical_crossentropy` as the loss function. We would also like to look at training and validation accuracy on each epoch as we train our network, so we are passing in the metrics argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Mg7_TXOVrWd"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YmQZ3TAVrWg"
      },
      "source": [
        "### Model Summary\n",
        "\n",
        "Let's look at all the layers of our network using **summary** method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vtny8hmBVrWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae4a7c4-7f5a-4d7e-85bd-aa802f7a594c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 48, 48, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 24, 24, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 22, 22, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 11, 11, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 9, 9, 128)         73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 2, 2, 128)         147584    \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 1, 1, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               66048     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 307,906\n",
            "Trainable params: 307,906\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N06iqE8VVrWj"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oub9RtoFVrWk"
      },
      "source": [
        "It's time we train our network.\n",
        "\n",
        "Since our batches are coming from a generator (`ImageDataGenerator`), we'll use `fit_generator` instead of `fit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSF2HqhDVrWk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "outputId": "e1281ca8-9a8b-4621-d90f-15125849d136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "5/5 [==============================] - 451s 92s/step - loss: nan - accuracy: 0.0320 - val_loss: nan - val_accuracy: 0.4000\n",
            "Epoch 2/100\n",
            "5/5 [==============================] - 330s 65s/step - loss: nan - accuracy: 0.0500 - val_loss: nan - val_accuracy: 0.4000\n",
            "Epoch 3/100\n",
            "5/5 [==============================] - 299s 60s/step - loss: nan - accuracy: 0.0460 - val_loss: nan - val_accuracy: 0.4000\n",
            "Epoch 4/100\n",
            "5/5 [==============================] - 271s 55s/step - loss: nan - accuracy: 0.0460 - val_loss: nan - val_accuracy: 0.4000\n",
            "Epoch 5/100\n",
            "5/5 [==============================] - 228s 46s/step - loss: nan - accuracy: 0.0480 - val_loss: nan - val_accuracy: 0.4000\n",
            "Epoch 6/100\n",
            "5/5 [==============================] - 191s 38s/step - loss: nan - accuracy: 0.0340 - val_loss: nan - val_accuracy: 0.4000\n",
            "Epoch 7/100\n",
            "5/5 [==============================] - 161s 32s/step - loss: nan - accuracy: 0.0417 - val_loss: nan - val_accuracy: 0.4000\n",
            "Epoch 8/100\n",
            "5/5 [==============================] - 143s 29s/step - loss: nan - accuracy: 0.0500 - val_loss: nan - val_accuracy: 0.4000\n",
            "Epoch 9/100\n",
            "5/5 [==============================] - 143s 29s/step - loss: nan - accuracy: 0.0479 - val_loss: nan - val_accuracy: 0.4000\n",
            "Epoch 10/100\n",
            "5/5 [==============================] - 130s 27s/step - loss: nan - accuracy: 0.0480 - val_loss: nan - val_accuracy: 0.4000\n",
            "Epoch 11/100\n",
            "5/5 [==============================] - 99s 20s/step - loss: nan - accuracy: 0.0540 - val_loss: nan - val_accuracy: 0.4000\n",
            "Epoch 12/100\n",
            "5/5 [==============================] - 109s 21s/step - loss: nan - accuracy: 0.0480 - val_loss: nan - val_accuracy: 0.4000\n",
            "Epoch 13/100\n",
            "5/5 [==============================] - 86s 18s/step - loss: nan - accuracy: 0.0500 - val_loss: nan - val_accuracy: 0.4000\n",
            "Epoch 14/100\n",
            "5/5 [==============================] - 77s 15s/step - loss: nan - accuracy: 0.0380 - val_loss: nan - val_accuracy: 0.4000\n",
            "Epoch 15/100\n",
            "5/5 [==============================] - 70s 14s/step - loss: nan - accuracy: 0.0480 - val_loss: nan - val_accuracy: 0.4000\n",
            "Epoch 16/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-d70e0a339f06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_val\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2272\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2273\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2274\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2276\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "EPOCHS = 100\n",
        "history = model.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=int(np.ceil(total_train / float(BATCH_SIZE))),\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=int(np.ceil(total_val / float(BATCH_SIZE)))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7Z2q14Zi_YrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojJNteAGVrWo"
      },
      "source": [
        "### Visualizing results of the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZPYT-EmVrWo"
      },
      "source": [
        "We'll now visualize the results we get after training our network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6oA77ADVrWp"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(EPOCHS)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.savefig('./foo.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDnr50l2VrWu"
      },
      "source": [
        "As we can see from the plots, training accuracy and validation accuracy are off by large margin and our model has achieved only around **70%** accuracy on the validation set (depending on the number of epochs you trained for).\n",
        "\n",
        "This is a clear indication of overfitting. Once the training and validation curves start to diverge, our model has started to memorize the training data and is unable to perform well on the validation data."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}